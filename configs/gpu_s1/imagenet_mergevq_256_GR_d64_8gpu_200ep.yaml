seed_everything: true
trainer:
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  devices: 8  # 8 GPU x 1 node x bs16 x 1 accumulation (manual optimization)
  num_nodes: 1
  # accumulate_grad_batches: 2  ### To setup manually
  precision: 16-mixed  # general GPU like V100 for fp16 training
  # precision: bf16-mixed  # Ampere-based GPU like A100 for better stability
  max_epochs: 200
  check_val_every_n_epoch: 1
  num_sanity_val_steps: -1
  log_every_n_steps: 100
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "./checkpoints/stage1/imagenet_mergevq_256_GR_d64_8gpu_200ep/"
        save_top_k: -1 # save the last 20 checkpoints
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: "./results/stage1/imagenet_mergevq_256_GR_d64_8gpu_200ep/"
      version: "test"
      name:

model:
  class_path: taming.models.lfqgan_rep.MergeVQModel
  init_args:
    ddconfig:
      double_z: False
      z_channels: 18
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 64
      ch_mult: [1,1,2,4,8,]  # num_down = len(ch_mult)-1
      num_res_blocks: 4
      num_att_blocks: 12
      num_heads: 8
      dist_head: 'layer3'
      num_classes: 768
      merge_num: 112  # kept 144 instead of 256
      cand_distribution: 'gaussian-6'  # [121, 144, 169, 196, 225, 256]
      model_path: 'vit_base_patch14_dinov2.lvd142m'
    k2lconfig:
      learn_source: False

    lossconfig:
      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
      params:
        disc_conditional: False
        disc_in_channels: 3
        disc_start: 0 # from 0 epoch
        disc_weight: 0.8
        gen_loss_weight: 0.1
        cls_loss_weight: 1.0
        lecam_loss_weight: 0.01
        codebook_weight: 0.1
        commit_weight: 0.25
        codebook_enlarge_ratio: 0
        codebook_enlarge_steps: 2000

    n_embed: 262144
    embed_dim: 18
    learning_rate: 1e-4
    sample_minimization_weight: 1.0
    batch_maximization_weight: 1.0
    scheduler_type: "None"
    use_ema: True
    resume_lr:
    lr_drop_epoch: [170, 190]
    pretrain_teacher: "facebook/dinov2-base"
    accumulate_grad_batches: 2  # 8 GPU x 1 node x bs16 x 2 accumulation (manual optimization)
    # gradient_clip_val: 5.0


data:
  class_path: main.DataModuleFromConfig
  init_args:
    batch_size: 16
    num_workers: 16
    train:
      target: taming.data.imagenet.ImageNetTrain
      params:
        config:
          size: [256, 224, 224]
          augmentation: ["default", "randomcrop", "default"]
          cachedir: ".cache/imagenet"
          subset:
    validation:
      target: taming.data.imagenet.ImageNetValidation
      params:
        config:
          size: [256, 224, 224]
          cachedir: ".cache/imagenet"
          subset:
    test:
      target: taming.data.imagenet.ImageNetValidation
      params:
        config:
          size: [256, 224, 224]
          cachedir: ".cache/imagenet"
          subset:

ckpt_path: null # to resume
